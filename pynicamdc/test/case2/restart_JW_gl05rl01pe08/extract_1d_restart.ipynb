{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67444714-2a99-4d40-90d4-37687b37fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON file saved as restart_all_GL05RL01z40.pe00000002.json\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# File path (update if needed)\n",
    "\n",
    "file_p = \"restart_all_GL05RL01z40.pe\"\n",
    "fnum=\"000002\"\n",
    "file_path = file_p + fnum\n",
    "output_json = file_p + \"00\" + fnum + \".json\"\n",
    "\n",
    "# Mapping from datatype code to NumPy dtype\n",
    "dtype_map = {0: \"f4\", 1: \"f8\", 2: \"i4\", 3: \"i8\"}\n",
    "\n",
    "# Initialize dataset storage\n",
    "dataset = {}\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    # Read metadata (header + note)\n",
    "    header = f.read(64).decode(errors=\"ignore\").strip()\n",
    "    note = f.read(256).decode(errors=\"ignore\").strip()\n",
    "\n",
    "    # Read global file metadata\n",
    "    fmode, endiantype, grid_topology, glevel, rlevel, num_of_rgn = struct.unpack(\">6I\", f.read(4 * 6))\n",
    "    \n",
    "    # Read region IDs\n",
    "    rgnid = struct.unpack(f\">{num_of_rgn}I\", f.read(4 * num_of_rgn))\n",
    "    \n",
    "    # Read number of data variables\n",
    "    num_of_data = struct.unpack(\">I\", f.read(4))[0]\n",
    "\n",
    "    # Dictionary to store extracted data\n",
    "    dataset = {\n",
    "        \"Header\": header,\n",
    "        \"Note\": note,\n",
    "        \"File Mode\": fmode,\n",
    "        \"Endian Type\": endiantype,\n",
    "        \"Grid Topology\": grid_topology,\n",
    "        \"Grid Level\": glevel,\n",
    "        \"Resolution Level\": rlevel,\n",
    "        \"Number of Regions\": num_of_rgn,\n",
    "        \"Region IDs\": list(rgnid),\n",
    "        \"Number of Data Entries\": num_of_data,\n",
    "        \"Variables\": {}\n",
    "    }\n",
    "\n",
    "    # Expected shape before reordering: (Region, Layer, ij)\n",
    "    expected_shape = (5, 42, 324)\n",
    "\n",
    "    # Process each data entry\n",
    "    for _ in range(num_of_data):\n",
    "        # Read variable metadata and remove null bytes\n",
    "        varname = f.read(16).decode(errors=\"ignore\").strip(\"\\x00\")\n",
    "        description = f.read(64).decode(errors=\"ignore\").strip(\"\\x00\")\n",
    "        unit = f.read(16).decode(errors=\"ignore\").strip(\"\\x00\")\n",
    "        layername = f.read(16).decode(errors=\"ignore\").strip(\"\\x00\")\n",
    "        note = f.read(256).decode(errors=\"ignore\").strip(\"\\x00\")\n",
    "\n",
    "        # Read integer fields\n",
    "        datasize, datatype, _, _ = struct.unpack(\">Q3I\", f.read(8 + 4 * 3))\n",
    "\n",
    "        # Read time information\n",
    "        time_start, time_end = struct.unpack(\">QQ\", f.read(8 * 2))\n",
    "\n",
    "        # Read raw data\n",
    "        raw_data = f.read(datasize)\n",
    "\n",
    "        # Convert to NumPy array in correct format (big-endian float)\n",
    "        data_array = np.frombuffer(raw_data, dtype=np.dtype(f\">{dtype_map[datatype]}\"))\n",
    "\n",
    "        # Ensure the correct number of elements before reshaping\n",
    "        if data_array.size != np.prod(expected_shape):\n",
    "            print(f\"Warning: Variable {varname} has unexpected size {data_array.size}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Reshape into (Region, Layer, ij)\n",
    "        data_array = data_array.reshape(expected_shape)\n",
    "\n",
    "## Ensure correct shape and convert Fortran (column-major) order to C (row-major) if needed\n",
    "#        reshaped_data = raw_data.reshape((5, 324), order='F').T if num_elements == 1620 else raw_data\n",
    "        \n",
    "        # Reverse dimensions to (ij, Layer, Region)\n",
    "        data_array = np.transpose(data_array, (2, 1, 0))\n",
    "\n",
    "        # Store full variable data\n",
    "        dataset[\"Variables\"][varname] = {\n",
    "            \"Description\": description,\n",
    "            \"Unit\": unit,\n",
    "            \"Layer Name\": layername,\n",
    "            \"Time Start\": time_start,\n",
    "            \"Time End\": time_end,\n",
    "            \"Data\": data_array.tolist()  # Store full dataset\n",
    "        }\n",
    "\n",
    "# Save full JSON file\n",
    "with open(output_json, \"w\") as json_file:\n",
    "    json.dump(dataset, json_file, indent=4)\n",
    "\n",
    "print(f\"✅ JSON file saved as {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d44b09-d8db-4769-8c21-3c3bf3531433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jax_mpi)",
   "language": "python",
   "name": "jax_mpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
